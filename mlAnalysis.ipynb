{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from matplotlib import pyplot\n",
    "from matplotlib import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os.path\n",
    "import csv\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "NAN_RATIO = 0.2 # Amount of NaN values to tolerate; otherwise, discard feature\n",
    "NUM_TOP_CAUSES = 4 # Only this amount of the top causes will be used in the classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a list of tuples of (causeName, amount of occurrences of causeName) \n",
    "# sorted by the amount of occurrences of causeName in decreasing order\n",
    "def getSortedCauses(df):\n",
    "    causes = {}\n",
    "    for index, row in df.iterrows():\n",
    "        if row['Cause'] not in causes:\n",
    "            causes[row['Cause']] = 1\n",
    "        else:\n",
    "            causes[row['Cause']] += 1\n",
    "        sortedCauses = sorted(causes.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sortedCauses\n",
    "\n",
    "# Return the data points of only the top NUM_TOP_CAUSES causes\n",
    "def getCommonCauses(df):\n",
    "    sortedCauses = getSortedCauses(df)\n",
    "    topCauses = [None]*NUM_TOP_CAUSES\n",
    "    for i in range(NUM_TOP_CAUSES):\n",
    "        topCauses[i] = sortedCauses[i][0]\n",
    "    print(topCauses)\n",
    "    dropIndices = []\n",
    "    for index, row in df.iterrows():\n",
    "        if row['Cause'] not in topCauses:\n",
    "            dropIndices.append(index)\n",
    "    df = df.drop(dropIndices)\n",
    "    return df\n",
    "    \n",
    "# Remove columns where there are many NaN values (based on NAN_RATIO)\n",
    "def filterData(df):\n",
    "    features = df.dtypes.index\n",
    "    feature = df[features[2]]\n",
    "    numNans = feature.isna().sum()\n",
    "    keepFeatures = []\n",
    "    for feature in features:\n",
    "        featureData = df[feature]\n",
    "        numNans = featureData.isna().sum()\n",
    "        if numNans < NAN_RATIO*N:\n",
    "            keepFeatures.append(feature)\n",
    "    return df[keepFeatures]\n",
    "\n",
    "# Filters the data for xgb use\n",
    "def filterForXGB(df):\n",
    "    df2 = df.drop(['OBJECTID', 'Accidno', 'Date', 'Street', 'CrossSt', 'X', 'Y', 'PtyAtFault', \n",
    "                   'PvtProp', 'Violation', 'PartyAge1', 'PartySex1', 'SafetyEq1', 'PartyAge2', \n",
    "                   'PartySex2', 'SafetyEq2', 'Distance'], axis=1)\n",
    "    df3 = filterData(df2)\n",
    "    df4 = df3.dropna()\n",
    "    return df4\n",
    "\n",
    "# Trains the XGB model for learning causes\n",
    "def trainCauseModel(df, restart=False, modelName='xgbCause.model'):\n",
    "    if not restart and os.path.isfile(modelName):\n",
    "        bst = xgb.Booster()\n",
    "        return bst.load_model(modelName)\n",
    "    else:\n",
    "        return trainXGB(df, modelName)\n",
    "\n",
    "# Uses the onehot_encode method to encode an entire training data\n",
    "def getEncodedTrainingData(df):\n",
    "    # Onehot encode for each categorical feature and remember the label_encoder\n",
    "    # The decodeDic should be useful in decoding the results\n",
    "    decodeDic = {} # {key=featureName, value=(len(onehot_vector), label_encoder)}\n",
    "    encodedTrain = None # Onehot encoded training data\n",
    "    lenOfEncodes = {} # This will store the length of each one-hot encode for each feature\n",
    "    for feature in df:\n",
    "        if feature == 'Cause': # Skip this because it's the label we're trying to guess\n",
    "            continue\n",
    "        column = df[feature]\n",
    "        if feature == 'Distance': # This doesn't need to be onehot encoded\n",
    "            decodeDic[feature] = (1, None)\n",
    "        elif feature == 'Time':\n",
    "            column = roundTime(column)\n",
    "        encoded, label_encoder, lenOfEncode = onehot_encode(column)   \n",
    "        lenOfEncodes[feature] = lenOfEncode\n",
    "        decodeDic[feature] = (len(encoded), label_encoder)\n",
    "        encoded = np.array(encoded)\n",
    "        \n",
    "        if encodedTrain is None:\n",
    "            encodedTrain = encoded\n",
    "        else:\n",
    "            encodedTrain = np.concatenate((encodedTrain, encoded), 1)\n",
    "            \n",
    "    return (encodedTrain, decodeDic, lenOfEncodes)\n",
    "    \n",
    "# Sets up the data to classify cause with XGB\n",
    "def trainXGB(df, modelName):\n",
    "    df = getCommonCauses(filterForXGB(df))\n",
    "    \n",
    "    # First remove all rows with cause=unknown\n",
    "    dropIndices = []\n",
    "    for index, row in df.iterrows():\n",
    "        if row['Cause'] == \"Unknown\":\n",
    "            dropIndices.append(index)\n",
    "    df = df.drop(dropIndices)\n",
    "    \n",
    "    # Encode all the training data\n",
    "#     print(df.shape)\n",
    "    encodedTrainX, decodeDic, lenOfEncodes = getEncodedTrainingData(df)\n",
    "#     print(encodedTrainX.shape)\n",
    "\n",
    "    # May need to convert label for causes to one-hot encoding rather than integer_encoding later\n",
    "    labelEncoder = LabelEncoder()\n",
    "    integerEncodedY = np.array(labelEncoder.fit_transform(df['Cause']))\n",
    "    \n",
    "    # Split data into train and test sets\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(encodedTrainX, integerEncodedY, test_size=0.05)\n",
    "#     print(X_train.shape)\n",
    "    \n",
    "    # Fit model to training data\n",
    "    model = XGBClassifier()\n",
    "    eval_set = [(X_test, Y_test)]\n",
    "    model.fit(X_train, Y_train, eval_metric=\"mlogloss\", eval_set=eval_set, verbose=False)\n",
    "    \n",
    "    # Make predictions for test data\n",
    "    Y_pred = model.predict(X_test)\n",
    "    predictions = [round(value) for value in Y_pred]\n",
    "    \n",
    "    # Evaluate predictions\n",
    "    accuracy = accuracy_score(Y_test, predictions)\n",
    "    print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))  \n",
    "    return (model, decodeDic, lenOfEncodes)\n",
    "\n",
    "# Uses model to predict unknown causes based on information entered.\n",
    "# Right now, this information is just taken from all the entries of the dataframe\n",
    "# that have \"unknown\" as the cause.\n",
    "# TODO: This needs work. The current problem is that since we are trying to predict\n",
    "# on unknown causes, the data we are predicting has the same amount of features as before,\n",
    "# but in these features, there are less different values, which would change the length\n",
    "# of the one-hot encoding, which prevents us from using model.predict \n",
    "def predictUnknownCauses(model, df, decodeDic):\n",
    "    df = getCommonCauses(filterForXGB(df))\n",
    "    dropIndices = []\n",
    "    for index, row in df.iterrows():\n",
    "        if row['Cause'] != \"Unknown\":\n",
    "            dropIndices.append(index)\n",
    "    df = df.drop(dropIndices)\n",
    "    dataToPredict, _, _ = getEncodedTrainingData(df)\n",
    "        \n",
    "    return model.predict(dataToPredict)\n",
    "\n",
    "# Converts the American time to military time\n",
    "def toMilitary(timeArr):\n",
    "    militaryTime = []\n",
    "    for time in timeArr:\n",
    "        parts = time.split(':')\n",
    "        hour = parts[0]\n",
    "        minute = parts[1]\n",
    "        if 'PM' in parts[2]:\n",
    "            hour = str(int(hour) + 12)\n",
    "            if hour == '24':\n",
    "                hour = '0'\n",
    "        militaryTime.append(':'.join([hour, minute]))\n",
    "    return militaryTime\n",
    "    \n",
    "# Rounds the time data to the nearest hour\n",
    "def roundTime(timeArr):\n",
    "    roundedTime = []\n",
    "    if 'PM' in timeArr.iloc[0] or 'AM' in timeArr.iloc[0]:\n",
    "        timeArr = toMilitary(timeArr.tolist())\n",
    "    for time in timeArr:\n",
    "        parts = time.split(':')\n",
    "        if int(parts[1]) >= 30:\n",
    "            if parts[0] is not '23':\n",
    "                roundedTime.append(str(int(parts[0]) + 1) + ':00')\n",
    "            else:\n",
    "                roundedTime.append('0:00')\n",
    "        else:\n",
    "            roundedTime.append(parts[0] + ':00')\n",
    "    return roundedTime\n",
    "    \n",
    "# Onehot encodes an array, or if label_encoder is supplied, decodes a onehot encoded vector\n",
    "def onehot_encode(arr, label_encoder=None):\n",
    "    if label_encoder is not None:\n",
    "        decoded = []\n",
    "        for onehot_vector in arr:\n",
    "            decoded.append(label_encoder.inverse_transform([np.argmax(onehot_vector)])[0])\n",
    "        return decoded\n",
    "    \n",
    "    # Integer encode\n",
    "    label_encoder = LabelEncoder()\n",
    "    integer_encoded = label_encoder.fit_transform(arr)\n",
    "\n",
    "    # Binary encode\n",
    "    onehot_encoder = OneHotEncoder(sparse=False, categories='auto')\n",
    "    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "    return (onehot_encoder.fit_transform(integer_encoded), label_encoder, len(np.unique(integer_encoded)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Traffic_Collisions.csv')\n",
    "N = len(df.index) # Number of data points\n",
    "K = len(df.dtypes.index) # Number of features\n",
    "df2 = filterForXGB(df)\n",
    "df3 = getCommonCauses(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model or load it if one exists already\n",
    "xgb, decodeDic, lenEncodeDic = trainCauseModel(df, restart=True)\n",
    "# Observation: Before adding the movement1 and movement2 features to the dataset, the\n",
    "# accuracy of the model was around 60%. After adding just movement1 and movement2,\n",
    "# the accuracy improved to around 77%. Adding the distance feature to that improved\n",
    "# the accuracy to around 80%. All this is with NUM_TOP_CAUSES=6\n",
    "# With NUM_TOP_CAUSES=4, accuracy is 92.98%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decodeDic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = xgb.feature_importances_\n",
    "arr = []\n",
    "j = 0\n",
    "for key in lenEncodeDic:\n",
    "    total = 0\n",
    "    for i in range(lenEncodeDic[key]):\n",
    "        total += feature_importances[j]\n",
    "        j += 1\n",
    "    arr.append((key, total))\n",
    "arr.sort(key=lambda x: -x[1])\n",
    "xArr = [None]*len(arr)\n",
    "yArr = [None]*len(arr)\n",
    "for i in range(len(arr)):\n",
    "    xArr[i] = arr[i][0]\n",
    "    yArr[i] = arr[i][1]\n",
    "plt.bar(xArr, yArr)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.title(\"Most Important Features in Determining Accident Causes\")\n",
    "plt.ylabel(\"Feature Importance\")\n",
    "plt.xlabel(\"Feature Name\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following 3 lines prints the num of unique values for each feature\n",
    "# for column in df2:\n",
    "#     print(column)\n",
    "#     print(len(pd.value_counts(df2[column])))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getSortedCauses(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis on Time of Day vs Pedestrian Involved\n",
    "df5 = df.drop(['OBJECTID', 'Accidno', 'Date', 'Street', 'CrossSt', 'X', 'Y', 'PtyAtFault', \n",
    "                   'PvtProp', 'Violation', 'PartyAge1', 'PartySex1', 'SafetyEq1', 'PartyAge2', \n",
    "                   'PartySex2', 'SafetyEq2', 'Distance'], axis=1)\n",
    "df5 = filterData(df5).dropna()\n",
    "roundedTimes = roundTime(df5['Time'])\n",
    "# print(roundedTimes)\n",
    "agg = [0]*24\n",
    "i = 0\n",
    "for index, row in df5.iterrows():\n",
    "    if \"No Pedestrian Involved\" not in row['PedAction']:\n",
    "        idx = int(roundedTimes[i].split(':')[0]) + 5\n",
    "        agg[idx % 24] += 1\n",
    "    i += 1\n",
    "\n",
    "xLabels = range(24)\n",
    "print(agg)\n",
    "plt.plot(xLabels, agg)\n",
    "plt.ylabel(\"Pedestrian Involved In Accident\")\n",
    "plt.xlabel(\"Time of Day\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
